
### 损失函数 Loss Function

**目的**：用于<font style="background-color:#FBF5CB;color:black">衡量</font> 模型预测值 与 真实数据标签 之间的<font style="background-color:#FBF5CB;color:black">差异</font>

**输入**： 是所有 **模型参数**

**输出**：是一个实数

损失函数**值的大小** 取决于 模型对训练数据的综合表现。


### 📉 梯度下降 Gradient Descent
梯度下降是一种优化算法，用来<font style="background-color:#FBF5CB;color:black">最小化损失函数</font>，从而<font style="background-color:#FBF5CB;color:black">不断调整</font>模型的参数（权重和偏置）让模型更“聪明”。

<font style="background-color:#FBF5CB;color:black">目标： 努力寻找 Loss 函数的全局最小值 </font>

梯度下降和反向传播的一些本质区别：
- 梯度下降负责更新参数，而反向传播只计算梯度，不负责更新参数
- 梯度下降一般没有层，不需要记录中间状态；反向传播需要记录每一层的输出

#### 梯度下降就是最小化 Loss 函数值
![](https://cdn.nlark.com/yuque/0/2025/png/295096/1744013423522-303b6fdb-20b1-4970-b111-ffc1d2929267.png)

假设 Loss 函数为 $y=f(w_1)$

理想情况下，<font style="background-color:#FBF5CB;color:black">梯度下降的目标是找到损失函数的</font><font style="background-color:#FBF5CB;color:black">全局最小值</font>（global minimum）, 如左图 $w_{min}$。

求 loss 函数极小值 (local minima) 就是找微分 (也就导数) 是 0 的 w 值.

在实际应用中，损失函数的形状可能很复杂，存在多个局部最小值或鞍点。因此，梯度下降<font style="background-color:#FBDE28;color:black">不一定</font>总是能<font style="background-color:#FBDE28;color:black">够找到全局最小值</font>，而是往往只能找到一个<font style="background-color:#FBF5CB;color:black">局部最小值（local minima）</font>或某个较低的优化点（saddle points）， 如右图 $w_{min}$

**📉 寻找 局部极小值 和 全局极小值**

![](https://cdn.nlark.com/yuque/0/2025/gif/295096/1744014262832-eefd5a4a-57dd-4c9f-9297-426ec776181a.gif)

计算点所在斜率，

斜率为正往左走，斜率为负往右走

斜率较大多走两步，快速下降；斜率较小， 少走两步；

最终找到 global minima

#### 多元函数的梯度下降
多元函数的<font style="background-color:#FBDE28;color:black">梯度</font>是一个向量，表示多元函数在一个点上 <font style="background-color:#FBDE28;color:black">变化最快的方向和变化率</font>。

![](https://cdn.nlark.com/yuque/0/2025/gif/295096/1744015367148-b4282be7-98cb-49d0-b2f4-df1483916057.gif)

#### 梯度下降的种类
+ **批量梯度下降（Batch Gradient Descent）**

每次使用所有的训练数据来计算梯度并更新参数。

+ **随机梯度下降（Stochastic Gradient Descent, SGD）**

每次使用一个样本来计算梯度并更新参数。

+ **小批量梯度下降（Mini-batch Gradient Descent）**

每次使用一小部分（mini-batch）数据来计算梯度并更新参数。


#### 梯度消失（Vanishing Gradient）
梯度消失是指在反向传播算法中，梯度的值在传递过程中逐渐变得非常小，尤其是在深层神经网络的前几层。

发生原因：

这种现象通常发生在使用激活函数（如Sigmoid或Tanh）时，因为这些函数在某些<font style="background-color:#FBF5CB;color:black">输入范围内的导数非常小</font>。当<font style="background-color:#FBF5CB;color:black">网络非常深</font>时，梯度在多次链式法则的作用下会不断衰减，最终变得非常接近零。

影响：

深层神经网络难以训练，尤其是当模型层数很深时，导致前几层的权重<font style="background-color:#FBF5CB;color:black">更新极其缓慢</font>，网络<font style="background-color:#FBF5CB;color:black">不能有效地学习</font>。
#### 梯度爆炸（Exploding Gradient）
梯度爆炸是指在反向传播过程中，梯度的值变得非常大，导致权重更新时出现极大的波动。

发生原因：

梯度爆炸通常发生在深度神经网络中，尤其是当使用某些激活函数（如ReLU）时。如果网络层之间的<font style="background-color:#FBDE28;color:black">权重初始化不当</font>，或者<font style="background-color:#FBDE28;color:black">激活函数的梯度过大</font>，可能会导致梯度在传递过程中指数级增长。

影响：

权重更新过大，导致数值溢出，模型参数无法收敛，训练过程变得非常不稳定。


#### 缓解梯度问题 （Mitigating Gradient Issues）
缓解梯度问题是指采取一系列方法来<font style="background-color:#FBF5CB;color:black">避免或减轻梯度消失和梯度爆炸的问题</font>，从而改善深度学习模型的训练过程。

常见的缓解办法：

+ **权重初始化**
+ **使用合适的激活函数**
+ **梯度裁剪（Gradient Clipping）**
+ **Batch Normalization**
+ **使用适应性优化算法**

#### 万能逼近定律（Universal Approximation Theorem）
万能逼近定律指出，对于任意的连续函数，只要神经网络的<font style="background-color:#FBF5CB;color:black">激活函数足够适合</font>（如sigmoid或ReLU等），并且网络<font style="background-color:#FBF5CB;color:black">有足够的隐藏层单元</font>（即足够的神经元），则<font style="background-color:#FBE4E7;color:black">神经网络就能逼近该函数的任意精度</font>。

意义：

1.  **通用性**：

该定理展示了神经网络的强大能力，意味着只要网络的容量（即隐藏层单元的数量）足够大，它就能逼近任何连续函数，理论上任何复杂的任务都可以用神经网络来解决。

2. **不依赖于具体问题**：

该定理表明，不论我们要解决的是回归问题、分类问题，还是其他形式的预测问题，只要神经网络有足够的能力，它都可以逼近目标函数。



### 优化器 Optimizer
<font style="background-color:#FBDE28;color:black">寻找某个目标函数的最优解</font>， 目标函数通常是 Loss 函数。

**输入**：模型参数

**输出**：优化函数的计算结果，用于衡量当前输入参数的优劣。

**包含**：

+ **目标函数 Objective Function**
+ **决策变量 Decision Variables**
+ **约束条件 Constraints**
+ **最优解 Optimal Solution**

#### SGD + Momentum

#### RMSProp

#### Adam




## 自动求导 (autograd)
[自动求导 jupyter-notes代码演示](../jupyter-notes/PyTorch-入门-自动求导等.ipynb)

回归 深度学习的核心思想：

模型训练的本质 = 不断更新参数（<font style="background-color:#FBDE28;color:black">权重和偏置</font>），使 <font style="background-color:#FBDE28;color:black">损失函数最小化</font>

—— 所以：我们要找<font style="background-color:#FBDE28;color:black">最优解（最小值）</font>

—— 找最小值：要用<font style="background-color:#FBDE28;color:black">梯度下降</font>

—— 梯度下降：要先求导

—— 求导过程：由 <font style="background-color:#FBDE28;color:black">自动微分/自动求导机制</font> 完成


自动微分是一种计算导数的技术，它通过记录**前向传播**过程中每个操作的<font style="background-color:#FBDE28;color:black">中间结果和操作本身</font>，然后反向逐步计算各个变量对目标函数的导数（<font style="background-color:#FBDE28;color:black">梯度</font>）。

<font style="background-color:#FBDE28;color:black">反向传播是实现自动求导的一种算法</font>（在神经网络中）。

#### 代码示例
```python
import torch

# 创建一个需要梯度的张量
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2  # y = x^2

# 计算梯度
y.backward()  # dy/dx = 2x
print(x.grad)  # 输出: tensor(4.) (因为 x=2, 2*2=4)
```

以下打开 github 代码查看：

+ 动态计算图
+ 梯度积累和清零
+ 阻止梯度跟踪
+ 高阶梯度


### 链式法则