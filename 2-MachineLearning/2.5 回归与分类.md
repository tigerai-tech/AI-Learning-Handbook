
## 回归

<font style="background-color: salmon; color:black">The process of going back to an earlier or less advanced form or state.</font>

### 概念



回归分析是一种用于<font style="background-color:salmon; color:black">研究变量之间关系</font>的统计方法，特别是研究一个$X$或多个自变量（也称为解释变量或预测变量）如何影响因变量$Y$（也称为目标变量或被预测变量）的变化。

<font style="background-color:yellow; color:black">回归 = 找出一个“输入”和“数值型输出”之间的规律，建立预测公式。</font>
如，用回归算法尝试估计 $y=2x$ 这个函数，然后预测x=n，y=？

#### 目标

- 总结函数：理解自变量对因变量的影响程度和方向
- 用于预测：预测在给定自变量值的情况下，因变量的可能取值

#### 核心结构
$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_n X_{ki} + \varepsilon$$
- y 是<font style="background-color:yellow; color:black">因变量</font>
- `x₁, x₂, ..., xₙ` 是<font style="background-color:yellow; color:black">自变量</font>
- `β₀, β₁, β₂, ..., βₙ` 是模型参数（也称为回归系数）
- ε 是<font style="background-color:yellow; color:black">误差项</font>，代表模型无法解释的随机变异
- <font style="background-color:yellow; color:black">拟合优度</font> $R^2$  衡量模型预测效果好坏的一个统计量


#### 回归线和回归面
![[Pasted image 20250414194030.png]]
![[Pasted image 20250415093020.png]]
回归线和回归面是**用数学模型对现实的一种近似抽象和拟合**，是一种“工具”，而不是客观存在的实体。
- **对数据的最佳线性拟合**，数据点本身并不都不在这条线/面上。


### 简单回归和多元回归

##### **简单回归（Simple  Regression）**
只包含**一个变量**
$$Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i$$

#####  多元回归（Multiple  Regression）
包含 **多个自变量**，**逐步控制、逐个研究**
$$Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots + \beta_k X_{ki} + \varepsilon_i $$

######  <font style="background-color:tomato; color:black">相关性陷阱</font> Correlation Pitfalls
在多元回归建模时，当心相关性陷阱

###### <font style="background-color:tomato; color:black">皮尔逊相关系数</font> Pearson Correlation Coefficient 
**Pearson皮尔逊相关系数**（Pearson Correlation Coefficient），衡量两个变量之间**线性相关程度**的统计量。

`pandas_data.corr()` 查看相关性系数

🧠： 两个变量 **X 和 Y <font style="background-color:yellow; color:black">是否线性相关</font>、相关性多强**，以及是**正相关还是负相关**。

公式：

$$r = \frac{\text{cov}(X, Y)}{\sigma_X \cdot \sigma_Y}  = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2} \cdot \sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}} $$
- $\bar{x}$：X 的均值
    
- $\bar{y}$：Y 的均值
    
- 分子是 **协方差**
    
- 分母是 **标准差的乘积**


### 线性回归
#### 简单线性回归 (Simple Linear Regression)
[SimpleLinearRegression-Jupyter Lab](./jupyter-notes/IBM-1-1-SimpleLinearRegression.ipynb)
用于预测一个连续的变量

- 最佳拟合 best fit
「最佳拟合」指的是找到一条最能描述 **自变量（X）** 和 **因变量（Y）** 之间关系的线（或平面），这个线就是我们要找的 **回归方程**。


#### 普通最小二乘法回归（OLS）

<font style="background-color:yellow; color:black">Ordinary Least Squares Regression</font>

OLS回归的特点：
- 易于理解和解释
- 不需要调参： 只需要提供数据
- 回归解是只需要通过计算得到的
- 准确值会受到 **异常值(outlier)** 的极大影响

为什么普通最小二乘法（OLS）回归在复杂数据集上的准确性有限？
对于复杂数据集而言，OLS可能无法捕捉到变量之间的非线性关系或高阶交互作用，因此其预测能力会受到限制。

#### 多元线性回归
[MultipleLinearRegression-Jupyter Lab](./jupyter-notes/IBM-1-2-MultipleLinearRegression.ipynb)
比简单线性回归相比，有<font style="background-color:yellow; color:black">多个输入变量</font>


#### 线性回归知识点

###### 1️⃣ <font style="background-color:tomato; color:black">残差平方和 SSR</font>
目标： 找到一条直线（或超平面），使得所有数据点的预测误差平方和最小。

一种线性回归的<font style="background-color:yellow; color:black">Loss function</font>
可用于衡量模型预测是否准确，回归线是否更贴合真实数据。

公式：

$$\text{SSR} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
- 实际值 $y_i$
- 预测值  $\hat{y}_i=β_0+β_1x_{i1}+⋯+βpxipy^​i​=β_0​+β_1​ x_{i1​}+⋯+βp​xip​$ 是预测值
- 残差（residual）是实际值与预测值之间的差异


 ###### 2️⃣ <font style="background-color: tomato; color:black">均方误差 MSE (Mean Squared Error)</font>
$$\text{MSE} = \frac{1}{n} \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \frac{\text{RSS}}{n}$$
均方误差MSE 是残差平方和的平均值


###### 3️⃣ 最小二乘解

**Least Squares Solution**（最小二乘解）的目标是：

> 找到一组参数，使得模型预测值与实际数据的**误差平方和**最小。

标准线性回归中，最优解就是最小二乘解（Least Squares Solution）。

*正则化模型（如岭回归、Lasso）或其他损失函数（如绝对误差）*，的最优解不是最小二乘解



### 非线性回归


##### 多项式回归 （polynomial regression）

> 用**多项式**（如 $y = a + bx + cx^2 + dx^3 + \cdots$）来拟合数据点，从而捕捉非线性趋势

###### 容易过度拟合
<font style="background-color:orange; color:black">多项式回归比线性回归更容易过拟合</font>（Overfitting） 
![[Pasted image 20250415173103.png]]
 
###### 本质上是线性回归
虽然多项式回归可以拟合非线性的曲线，但**它在本质上依然是线性的**。

多项式回归是<font style="background-color: yellow; color:black">非线性回归</font>，但一般可以转化成线性回归。
![[Pasted image 20250415163617.png]]
> <font style="background-color:orange; color:black">多项式回归是线性回归的一种扩展，只要它在线性模型中对输入做了非线性变换。</font>


##### 其他非线性回归 (Nonlinear Regression)
![[Pasted image 20250415163345.png]]
- 用 <font style="background-color:orange; color:black">非线性方程式</font> 表示
- 方程式可以是： 多项式，指数, 对数或非线性函数
- 用于复杂关系回归

🌰 Examples:
1. 中国GDP的指数级增长不能使用线性回归来分析

![[Pasted image 20250415164624.png]]
2. 工作时长与生产力的对数增长回归曲线
![[Pasted image 20250415164549.png]]

其他非线性回归：
- 回归树
- 随机森林
- 神经网络
- 支持向量机 SVM
- 梯度提升机 Gradient Boosting Machine
- K近邻居 KNN

#### 数据可视化
如何知识我该采用哪种回归建模？

#####  **散点图**（scatter plot）
#####  **变换线性化法（Transformation）**
如果你怀疑是非线性关系，可以尝试“变换”数据让它线性化：

- 如果怀疑是 **对数关系**：尝试 log(Y) 或 log(X) 再做线性回归
    
- 如果怀疑是 **指数关系**：尝试 log(Y) 对 X 做线性回归
    
- 如果怀疑是 **幂函数关系**：尝试 log(Y) 对 log(X) 回归
##### **残差图分析（Residual Plot）📈**

- 建模后，画出 **预测值 vs 残差图**
    
- 如果残差呈现某种**系统性趋势**（如曲线、波动），说明线性模型不合适
    
- 随机散布的残差 → 模型可能合适
    
##### **拟合优度指标（R²、AIC、BIC、MSE 等）📊**

- 拟合不同模型后，比较这些指标：
    
    - **R² 越接近 1 越好**
        
    - **AIC/BIC 越低越好**
        
    - **MSE 越小越好**
  

这些变换后，再用 **Pearson相关系数** 检查是否变得更线性。



### 逻辑回归 Logistic Regression

❌ 不是非线性回归方法，也不是回归模型，它是一个**线性模型**，预测**某个事件发生的概率**，用于解决**分类**问题

#### <font style="background-color:tomato; color:black">sigmoid 函数</font>

> <font style="background-color:yellow; color:black">把一个线性回归的输出，通过sigmoid函数转换成一个0到1之间的概率，然后根据这个概率来进行分类判断。</font>

数学表达式：

假设线性回归的表达： $z = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n$
那么在逻辑回归中：
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$
这个函数就是 sigmoid函数
![[Pasted image 20250415202645.png]]


#### <font style="background-color:tomato; color:black">交叉熵损失函数（cross-entropy）</font>

 **🎯 用来衡量模型预测值和真实标签之间的差距，是逻辑回归的损失函数！**

| **名称**                                                             | **通常应用场景** | **是否等价**          |
| ------------------------------------------------------------------ | ---------- | ----------------- |
| <font style="background-color:tomato; color:black">Log-Loss</font> | 二分类        | ✅ 是交叉熵在二分类下的形式    |
| **Cross-Entropy**                                                  | 多分类更常见     | ✅ 广义形式（支持二分类和多分类） |

逻辑回归的损失函数是 **交叉熵损失函数**（cross-entropy）

表达式： 
$$\text{Loss} = -\sum_{i=1}^n \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]$$

#### 什么时候用逻辑回归？
- 二分类问题
- 当需要计算输出结果的概率
- 如果数据是线性可分的，逻辑回归的决策边界是一条直线，一个平面或超平面（n-1维结构）。

#### 应用
- 预测心脏病风险
- 基于一系列特征诊断患者
- 预测顾客是否会购买
- 预测产品失败的可能性




## 分类问题 Classification


### 基础

#### 概念

**学习一个模型，能够根据输入数据的特征（属性），将其正确地归入预定义的、离散的类别 (Class) 或标签 (Label) 中**

分类问题 是[监督学习](./2.2%20机器学习的分类.md) 的一种核心问题。

#### 应用
- Email 过滤
- Speech-To-Text
- 书法识别
- 生物识别
- 分档分类
- 客户服务
	- 客户留存预测
	- 客户分类
	- 广告响应
- 贷款违约
- 多分类药物处方

#### 常用分类算法

🥏 [逻辑回归 Logistic Regression](#逻辑回归-logistic-regression)

🥏 朴素贝叶斯 Naive Bayes

🥏 支持向量机 Support Vector Machine

🥏 [决策树 Decision Tree](#-决策树-decision-tree)

🥏 随机森林 Random Forest

🥏 K近邻 K-Nearest Neighbors, KNN

🥏 神经网络 Neural Networks


#### 多重分类 Multi- Class Classification


##### 多重分类策略

> 目的是把原本只能处理二分类问题的**分类器**，**扩展**到可以处理**多分类问题。**


假设你要分类「猫 🐱」、「狗 🐶」、「鸟 🐦」这三类：

 🛝 One-Versus-All
如果有K个类型，就训练K个二分类器(Binary Classifier)

- 训练3个分类器：
    
    - 分类器1：<font style="background-color:yellow; color:black">猫 vs 非猫</font>（狗+鸟）
        
    - 分类器2：狗 vs 非狗（猫+鸟）
        
    - 分类器3：鸟 vs 非鸟（猫+狗）

预测：把样本分别输入到K个分类器中，选择概率最高的类别


🛝 One-Versus-One
如果有K个类型，就每两个类型训练一个分类器，共训练$\frac{k*(k-1)}{2}$个分类器

- 训练3个分类器（3 对 2 组合）：
    
    - 分类器1：<font style="background-color:yellow; color:black">猫 vs 狗</font>
        
    - 分类器2：<font style="background-color:yellow; color:black">猫 vs 鸟</font>
        
    - 分类器3：狗 vs 鸟

预测：多数投票法。每个分类器投票，谁赢的次数最多就归为谁


### 🥏 决策树 Decision Tree 
[动手实验-决策树](../jupyter-notes/IBM-1-2-5-DecisionTree.ipynb)

这里主要讨论决策树的 分类树分支， 回归树也是决策树的一种。

#### 1. 🧠 概念定义

🌰 是否接Offer？
![[Pasted image 20250416192357.png]]

> 1. 决策树是通过**递归地把数据集分割成更小的部分**，来构建一棵“树”，最终用于对数据进行分类。
> 2. 在训练决策树时，每一步都要**选择一个“最能把数据分得清楚”的最好的特征**来做划分。

本质是一棵<font style="background-color:yellow; color:black">树形图</font>，用来表示决策过程，树的叶子节点，代表一个分类

用于数据分类 或 预测结果



<font style="background-color:tomato; color:black">"Tree Pruning"（决策树剪枝）</font>
在决策树学习完成后，对其结构进行修改以减少其复杂性、提高其泛化能力并提升其可解释性的过程。

**为什么需要剪枝？**

1. **过拟合 (Overfitting):** 这是最主要的原因。未经过剪枝的决策树可能会过度学习训练数据的细节和噪声，导致决策边界过于复杂。
2. **提高泛化能力:** 简单的模型通常具有更好的泛化能力，能更好地处理新数据。
3. **提高效率和可解释性:** 较小的树在预测时需要的计算量更少，也更容易理解和解释。
4. **减少偏差:** 有时剪枝可以在一定程度上降低模型的偏差，使其不那么“天真”。


<font style="background-color:tomato; color:black">停止条件（Stopping Criteria）</font>
不再继续分裂节点的条件
- 达到最小树高
- 当前节点样本数 小于 最小样本数
- 叶子节点数量达到最大数量
-  某个叶子节点中，**样本数少于你设置的最小值**


<font style="background-color:tomato; color:black">选择最佳分裂点的标准</font>
如何测量最好的特征：

- 🍏 **信息增益 (Information Gain):**

熵越低，信息增益越大，表示分裂后数据的不确定性减少得越多，分类结果越清晰

$$\text{Information Gain} = H(D) - \sum_{i=1}^{k} \frac{|D_i|}{|D|} H(D_i)$$
- k, 有k个类别   
    
- H(D)：当前数据集的熵
    
- $D_i$：按某个特征划分后的子集
    
- $H(D_i)$：子集的熵
    
- $\frac{|D_i|}{|D|}$：子集所占的比例（加权）



- 🍏 **基尼不纯度 (Gini Impurity)**

衡量从一个数据点随机地选取两个样本，其标签不一致的概率.
基尼不纯度越低，表示数据集越纯净（即样本标签越一致）

$$\text{Gini} = 1 - \sum_{k=1}^{K} p_k^2$$

- K 个类别 
    
- $p_k$ 第 k 类在当前节点中的比例

#### ✨ 2. 决策树的优势

- 模型可视化
- 可解释性强
- 对特征工程要求低
	- 不需要特征缩放，如不需要fit_transform
	- 自动选择重要特征
- 灵活的决策


#### 3. 类别

| **类型**                       | **用于任务**   | **输出结果**  | **举例**      |
| ---------------------------- | ---------- | --------- | ----------- |
| **分类树（Classification Tree）** | 分类任务（分类标签） | 类别（离散值）   | 是/否、红/绿/蓝 等 |
| **回归树（Regression Tree）**     | 回归任务（预测数值） | 连续的数值（实数） | 房价预测、体重预测等  |

#### 4. 🥏 回归树 Regression Tree
[回归树-DEMO](../jupyter-notes/IBM-1-2-6-DecisionTree-RegressionTree.ipynb)

是决策树的一种
当决策树适用于解决回归问题时， 这种决策树就是回归树

##### 如何创建

- 递归分割数据集，直到达到 最大信息收益。
- 减少分裂时数据类别的不确定性或随机性

##### 数据分割规则

- 最小化真实值与预测值之间的差异
- MSE 