
**特征工程**（Feature Engineering）是机器学习和深度学习中的一个非常重要的步骤，旨在通过对数据的处理和转换，帮助模型更好地学习到数据中的模式和规律。特征工程的主要任务包括**特征提取**、**特征选择**和**特征构造**。

## 1️⃣ 特征工程 Feature Engineering
### 特征 Feature
**<font style="background-color:tomato;color:black">特征 = 事物的特点</font>**

特征 Feature，是指从原始数据中提取出来的、用于表示样本的**属性和信息**。

<font style="background-color:#FCE75A;"></font>

在机器学习模型中，特征被用作模型的输入，用于表示样本的不同属性，从而帮助模型进行学习和预测。因此，<font style="background-color:yellow;color:black">需要将数据表现为一组特征，而后才能将其输入，进行机器学习。</font>


### 特征工程 Feature Engineering


**<font style="background-color:#FCE75A;color:black">特征工程 = 把原始数据变成有用的特征</font>**

浅层学习通常包括传统的学习方法，如逻辑回归、决策树、支持向量机、朴素贝叶斯等，以及一些非线性模型，如 K 近邻 和 随机森林等。

这些方法通常基于人工设计的特征，即特征工程， 通过对特征进行处理和选择来构建模型。

<font style="background-color:#FCE75A;"></font>

🌰原始数据："2024-05-20购买"  
特征工程后 → "购买季节：春季"、"购买距今天数：300"、"是否周末：是"


### 特征工程的步骤

1.    **数据理解**：首先，需要了解数据的类型、分布以及与目标变量的关系。
2.    **特征提取**：从原始数据中提取重要的特征（例如图像、文本、时间序列数据）。
3.    **特征选择**：选择对预测最有帮助的特征，去除冗余或不相关的特征。
4.    **特征构造**：通过现有特征创建新的、更有意义的特征，增加模型的表现力。
5.    **特征变换**：对特征进行标准化、归一化或对数变换等操作，以提高模型性能。



#### 数据理解 Data Understanding
**数据理解**（Data Understanding）是数据分析和机器学习流程中的第一步，目的是通过探索和分析数据来了解其<font style="background-color:#FBDE28;color:black">结构、性质、分布、潜在的关系、质量</font>等。这一阶段为后续的数据预处理、特征工程和模型选择提供基础。

1. **数据收集（Data Collection）**

数据库，文件，API，传感器，网页抓取等

2. **数据预览（Data Preview）**

查看数据基本结构、数据类型、字段分布等

3. **数据质量检查（Data Quality Check）**

识别缺失、重复、异常值等问题

4. **数据分布和关系探索（Exploring Data Distribution and Relationships）**
    - 探索单一特征的分布
    - 探索特征之间的关系

目的：识别重要特征、潜在模式、数据分布异常情况及特征间的关系

5. **数据清理（Data Cleaning）**
    - 处理缺失值
    - 处理重复值
    - 处理异常值
    - 转换数据类型
6. **数据转换（Data Transformation）**
    - **标准化和归一化** 将数据缩放到统一范围，确保对模型特征的影响是平等的
    - **对数变换和平方根变换** 减少偏态数据的影响，使数据更符合正态分布
    - **编码分类数据 **将编码型数据变换到数值型数据
7. **目标变量分析（Target Variable Analysis）**
    - **分类问题** 分析类别的分布是否均匀
    - **回归问题** 分析目标变量的分布，是否是正态分布，是否存在极端值
8. **特征工程前的理解总结**
    - 数据分布
    - 数据质量问题
    - 特征间的关系
    - 目标变量分析



#### 特征提取 Feature Extraction
特征提取是从原始数据中<font style="background-color:#FBDE28;color:black">提取出有用的信息</font>，以便机器学习模型可以使用这些信息进行训练和预测。



1️⃣ **图像特征提取**：

+ **边缘检测**：例如使用Canny边缘检测算法，提取图像中的边缘特征。
+ **SIFT、SURF、ORB**：用于提取局部特征点，广泛用于计算机视觉领域。
+ **卷积神经网络（CNN）**：通过卷积层自动提取图像特征，是现代图像识别方法的基础。



**2️⃣**** 文本特征提取**：

+ **TF-IDF（Term Frequency-Inverse Document Frequency）**：衡量一个词语在文本中的重要性，用于文本分类和信息检索。
+ **Word2Vec、GloVe**：这些方法通过词嵌入将词语转换为向量，并捕捉词语之间的语义关系。
+ **BERT、GPT**：基于深度学习的语言模型，能够提取上下文信息，捕捉更丰富的文本特征。



**3️⃣**** 时间序列特征提取**：

+ **移动平均**：平滑时间序列数据，去除噪声。
+ **傅里叶变换**：提取周期性信号的频域特征。
+ **自回归模型（AR）、滑动平均模型（MA）**：用于提取时间序列中的依赖关系。



#### 特征选择 Feature  Selection
特征选择是从所有可用特征中<font style="background-color:#FBDE28;color:black">选择出最重要、最具信息量</font>的特征，从而提高模型的性能，避免过拟合，并减少计算成本。

**特征选择方法：**

**1️⃣ 过滤方法（Filter Methods）**：

+ 通过计算每个特征与目标变量之间的相关性（如皮尔逊相关系数、卡方检验、信息增益等），选择与目标变量相关性强的特征。
+ 优点：计算简单，适用于大规模数据集。
+ 缺点：忽略特征之间的相互关系。

**2️⃣ 包裹方法（Wrapper Methods）**：

+ 使用机器学习模型评估特征子集的性能，常见的算法有递归特征消除（RFE）。它通过训练模型来选择最优特征子集。
+ 优点：能够考虑特征间的相互作用。
+ 缺点：计算开销大，适用于较小的数据集。

**3️⃣ 嵌入方法（Embedded Methods）**：

+ 这种方法将特征选择过程嵌入到模型训练中。常见的嵌入方法有Lasso回归（L1正则化）和树模型（如随机森林、梯度提升树）。
+ 优点：自动选择重要特征，避免了过拟合。
+ 缺点：需要模型支持，计算开销较大。



**常用的特征选择算法：**

+ **L1正则化（Lasso回归）**：通过对特征加上L1正则化，迫使一些特征的系数变为零，从而实现特征选择。
+ **随机森林**：通过评估特征的重要性来选择最有影响力的特征。
+ **主成分分析（PCA）**：虽然主要用于降维，但它也可以作为一种特征选择技术，选择最大方差的主成分。



#### 特征构造 Feature Construction
特征构造是通过现有特征<font style="background-color:#FBDE28;color:black">创建新的特征</font>，这些新的特征可能更加具有代表性，有助于模型更好地学习数据中的模式。



**特征构造方法：**

**1️⃣ 数学运算**：

对现有特征进行加、减、乘、除等运算，生成新的特征。例如，在房价预测中，可能会通过房屋面积和房间数量计算每平方米价格作为新的特征。

**2️⃣ 组合特征**：

将多个特征组合成一个新的复合特征。例如，在银行信用评分中，可以通过将客户的年龄和收入组合成一个“收入年龄比”作为新的特征。

**3️⃣ 时间衍生特征**：

在时间序列数据中，通过现有的时间戳特征创建新的时间相关特征。例如，从“时间”特征中提取出“星期几”、“月份”或“季度”等信息。

**4️⃣ 分类特征转换**：

将连续变量划分为离散区间（如年龄分段），或将分类特征编码为数值特征（如One-Hot编码）。

5️⃣ **聚合特征**：

对于具有多行的类别数据，可以计算每个类别的统计信息（如均值、标准差、最大值、最小值等），作为新的特征。



#### 特征变换 Feature Transformation

**特征变换（Feature Transformation）** 是对数据特征进行某些数学或统计<font style="background-color:#FBDE28;color:black">处理</font>的过程，旨在使特征更加适合于机器学习模型的训练。这些变换通常可以提高模型的表现，改善模型的收敛速度，或者帮助模型处理不同的数据分布。

**常用特征变换方法：**

1. **标准化（Standardization）**

将特征转换为均值为 0，标准差为 1 的分布。常用于将特征<font style="background-color:#FBDE28;color:black">缩放到统一的尺度</font>，以确保所有特征对模型的影响是平等的。 $x{\prime} = \frac{x - \mu}{\sigma}$

**适用情况**：适用于大多数基于距离的算法（如 KNN、SVM、线性回归）和基于梯度下降优化的算法（如神经网络）。

2. **归一化（Normalization）**

归一化是将特征<font style="background-color:#FBDE28;color:black">缩放到一个固定的范围</font>内，通常是 [0, 1] 或 [-1, 1]。归一化通过减小特征值的差异，使得所有特征都处于相同的尺度上，帮助一些对特征值范围敏感的算法（如神经网络、KNN 等）更好地训练。

$x{\prime} = \frac{x - \min(x)}{\max(x) - \min(x)}$

**适用情况**：适用于对距离敏感的算法，如 KNN、支持向量机（SVM）等。

3. **对数变换（Log Transformation）**

对数变换用于将具有<font style="background-color:#FBDE28;color:black">指数增长或长尾分布</font>的特征转化为更加符合正态分布的形式。它对于处理具有较大差异的特征（例如收入、价格等）非常有效，能够减轻异常值的影响。

$x{\prime} = \log(x + 1)$

**适用情况**：适用于数据存在较大偏差或非线性关系的情况。

4. **平方根变换（Square Root Transformation）**

平方根变换类似于<font style="background-color:#FBDE28;color:black">对数变换</font>，旨在将具有较大差异的特征值转化为更接近正态分布的形式，但它通常比对数变换更温和。

$x{\prime} = \sqrt{x}$

**适用情况**：适用于数据存在轻微偏差或较少的异常值。

5. **Box-Cox 变换**

Box-Cox 变换是一种广泛用于使数据更接近<font style="background-color:#FBDE28;color:black">正态分布</font>的技术。它的目的是找到一个最佳的幂变换，使得数据更加符合正态分布。

$x{\prime} = \frac{x^\lambda - 1}{\lambda} \quad \text{if} \quad \lambda \neq 0$ ,    $x{\prime} = \log(x) \quad \text{if} \quad \lambda = 0$

**适用情况**：当数据分布非常不对称时，可以尝试使用 Box-Cox 变换来使数据分布更接近正态分布。

6. **离散化（Discretization）**

离散化是将连续的数值特征转换为<font style="background-color:#FBDE28;color:black">离散的类别特征</font>。例如，可以将一个连续的年龄特征转化为年龄段（如 0-18、19-35、36-50、50+）来减少数据的复杂度。

**方法**：

	•	等宽离散化：将数据范围均分为若干个区间。

	•	等频离散化：将数据按频率划分成若干个区间，使每个区间内的样本数量相同。

**适用情况**：适用于将连续变量转换为类别变量，特别是在某些分类模型中可能会提高表现。

7. **主成分分析（PCA, Principal Component Analysis）**

主成分分析是一种<font style="background-color:#FBDE28;color:black">降维技术</font>，旨在通过线性变换将原始特征空间投影到一个新的空间，从而<font style="background-color:#FBDE28;color:black">减少特征的维度</font>，同时保留数据的最大方差。这是一种非常有效的特征变换方法，尤其在处理高维数据时。

适用情况：当数据集中的特征维度过高时，可以使用 PCA 降维，减少计算量并防止过拟合。

8. **One-Hot 编码（One-Hot Encoding）**

One-Hot 编码是一种用于将类别特征转化为<font style="background-color:#FBDE28;color:black">数值特征的方法</font>。它将每个类别转换为一个二进制特征，若某个类别出现则为 1，否则为 0。

**适用情况**  适用于处理离散的类别特征，特别是当类别较少时





## 2️⃣ 表示和表征

#### 表示 Presentation
相当于  <font style="background-color:tomato;color:black">表示 = 把原始数据转化成计算机更好识别的数据</font>

例子：

    - 猫 , cat -> 数字符号 0101
    - 图片 转化 成 28*28 的向量矩阵

#### 表征 Presentation/Embedding

相当于 <font style="background-color:tomato;color:black">表征 = 特征提炼 = 在表示的基础上进一步提取有价值的信息</font>

在深度学习中，表征通常是由**模型自动学习**得到的。

例子：
-  猫 通过 AI 训练后，变成向量 [0.8, 0.1, -0.3]，其中 0.8 表示动物属性，0.1 表示个头偏小
- 人脸图片 -> 用深度学习模型提取出 【眼睛间距】【鼻梁角度】等隐含特征

<font style="background-color:honeydew;color:black">注意在机器视角里，人脸图片是向量矩阵，眼睛间距对机器而言是隐含特征。</font>

#### 局部表示 Local Representation

<font style="background-color:#FCE75A;color:black">局部表示也称 离散表示 或 符号表示</font>

**独热编码（One-Hot）是典型例子**

    - 比如用`[1, 0, 0]`表示“猫”，`[0, 1, 0]`表示“狗”，[0, 0, 1] 表示“鸟”。
    - 每个向量只有一个位置是1，其余全0，像开关一样“激活”某个特定概念。

特点：

    - 可解释性强
    - 离散且互斥

#### 分布式表示 Distributed Representation
分布式表示是一种将<font style="background-color:tomato; color:black">数据表示为多维向量</font>的方法，其中每个维度都包含有关数据的一部分信息。

分布式表示<font style="background-color:tomato;color:black">通常可以表示低维的稠密向量</font>，相当于让所有颜色的名字构成一个词表。

![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743424511318-7e91db5b-538e-4a81-8e19-78e7ec20a0ae.png)

例如：

+ 颜色的 RGB 表示
+ 在自然语言处理中，<font style="background-color:yellow;color:black">分布式词向量（如Word2Vec、GloVe等）将单词表示为多维向量</font>，每

个维度都包含了单词在不同语义和语法属性上的信息，从而在词汇表很大的情况下，能够更好地表示

单词之间的语义和语法关系。



<font style="background-color:salmon;color:black">神经网络中使用嵌入层 Embedding Layer 对输入数据进行分布式表示。</font>

<font style="background-color:#F1A2AB;"></font>

### 表征学习 Representation Learning
自动化地从原始数据中学习有效特征表示（即**表征**）的方法。

表征学习能够从数据中提出更丰富、高级的特征。

<font style="background-color:lightblue;color:black">要想学到一种好的高层语义表示，通常需要从底层特征开始，经过多步骤非线性转换才能得到。</font>

#### 常见方法
1. **监督学习下的表征学习**
    - 模型在训练过程中隐含地学习表征。例如：
        * 卷积神经网络（CNN）的中间层输出可作为图像的表征。
        * Transformer的隐藏状态可作为文本的表征。
2. **无监督/自监督表征学习**
    - 无需标注数据，通过数据内在结构学习表征：
        * **自编码器（Autoencoder）**：通过重构输入数据学习压缩表征。
        * **对比学习（Contrastive Learning）**：如SimCLR、MoCo，通过拉近相似样本、推开不相似样本学习表征。
        * **生成模型**：如GAN、VAE，通过生成数据的过程学习表征。
3. **迁移学习与预训练模型**
    - 在大规模数据上预训练模型（如BERT、ResNet），其学到的表征可迁移到小样本任务中，显著提升性能。





## 3️⃣ 浅层学习 Shallow Learning
![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743418953655-053a2dab-6775-4ab1-800b-2cc067578735.png)

<font style="color:#181a1f;"></font>

<font style="background-color:#FBDE28;color:black">根据网络分层深度和复杂性分类，可将机器学习分为浅层学习 Shallow Learning 和 深度学习 Deep Learning.</font>

### 浅层学习和深度学习的区别


**1️⃣ 浅层学习**：

<font style="background-color:tomato;color:black">只有一层或者少数几层的模型</font> 例如线性回归、决策树、支持向量机（SVM）等等

**<font style="background-color:skyblue;color:black">不涉及自动特征学习</font>**，特征主要**依赖人工经验** 或 <font style="background-color:tomato; color:black">特征工程</font>（如 PCA 降维、手工设计文本特征等）。

适用于**特征明确、数据量较小**的问题。

<font style="background-color:yellow">浅层学习同样需要通过训练来学习数据中的模式，只是模型本身比深度学习模型简单得多，层数较少。</font>
 

**2️⃣ 深度学习**：

通常具有<font style="background-color:tomato; color:black">多层神经网络结构</font>（通常指 3 层及以上），如 **DNN、CNN、RNN、Transformer**等

**可以<font style="background-color:tomato; color:black">自动学习复杂特征</font>**，无需人工设计特征，如 CNN 可自动提取图像特征，RNN 可自动学习时间序列模式。

适用于 **复杂数据（如图像、语音、文本）、大规模数据集、非线性问题**。




## 4️⃣ 深度学习

### 深度学习定义

<font style="background-color:#FBDE28;color:black">使用深层次、复杂结构</font>的神经网络模型进行学习的方法。

深度学习模型可以通过在多个层次上学习到更抽象、高级 的特征表示，从而能够更好捕捉数据中的内在结构和特征。

![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743429779016-4bee7c5f-00d5-444b-9f56-d03892bc49be.png)



### 端到端学习 End-to-End Learning

从输入 到 输出<font style="background-color:#FBDE28;color:black">整体建模</font>，而不需要拆分多个步骤或手工设计特征。

核心思想： 让模型自动学习<font style="background-color:#FBDE28;color:black">从原始数据到目标任务的完整映射</font>，减少人为<font style="background-color:#FBF5CB;color:black">干预和模块设计的复杂性</font>。

<font style="background-color:#FBE4E7;color:black">目前，大部分采用神经网络模型的深度学习也可以看作是一种端到端的学习。</font>


举例：

+ **传统方法**：通常需要分阶段处理，例如：
    - **语音识别**：音频 → 特征提取（MFCC）→ 声学模型 → 语言模型 → 文本输出。
    - **目标检测**：图像 → 候选区域生成（如Selective Search）→ 特征提取 → 分类 + 回归。
+ **端到端方法**：直接建模整个流程：
    - **语音识别**：音频 →（神经网络）→ 文本。
    - **目标检测**：图像 →（神经网络）→ 边界框 + 类别。





