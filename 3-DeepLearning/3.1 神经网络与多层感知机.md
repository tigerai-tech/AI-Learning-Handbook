

## 神经网络 Neural Network

神经网络（Neural Network）是人工智能中的一种**模拟人脑神经系统**的计算模型，它通过大量简单的“神经元”连接在一起，**模仿人脑的学习和思考方式**来处理复杂的数据和任务。



**🧠 通俗一点说：**

神经网络好比一个巨大的**函数**，它通过**训练**学习输入和输出之间的关系，然后就能**“推理”或“预测”**新的结果。



### 神经网络基本结构
#### 输入层 Input Layer
接收原始数据特征

#### 隐藏层 Hidden Layer
每一层由多个神经元（节点）组成，每个节点都与前一层的所有节点相连。隐藏层数量越多，模型越“深”。

隐藏层 对输入数据进行特征提取和转换，从而实现复杂的非线性映射。

如果每层的每一个神经元 都和另一层的所有神经元都相连，这就是 **<font style="background-color:#FBDE28;color:black">全连接层 Full Connected Layer</font>**。



#### 输出层 Output Layer
用于输出预测结果，比如分类标签或回归值。



### 神经元 Neuron
神经元是神经网络的最小组成单位，其本质是一个**<font style="background-color:#FBDE28;color:black">函数组合</font>**。



神经元是用于模拟人工神经单元的计算单元，用于构建**人工神经网络（ANN）。**



#### 工作原理
+ 线性变换 （加权求和）

$z = Wx + b$

+ 非线性映射 (激活函数)

常用的有 Relu， Sigmoid，Tanh 等，使模型处理非线性



#### 什么是激活函数？
激活函数（Activation Function）是神经网络中用于引入非线性因素的函数，目的是让神经网络能够学习和表达复杂的模式。



常见的激活函数：

1.    **Sigmoid函数**：将输入值压缩到0和1之间，常用于二分类问题。 

$\sigma(x) = \frac{1}{1 + e^{-x}}$
2. **ReLU（Rectified Linear Unit）函数**：输入大于0时输出输入值，否则输出0。它在深度学习中被广泛使用，因为其计算简单，且有助于缓解梯度消失问题。

$\text{ReLU}(x) = \max(0, x)$

3. **Tanh函数**：将输入值压缩到-1和1之间，类似于Sigmoid，但输出范围更大，常用于循环神经网络（RNN）。

$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$

4. **Softmax函数**：通常用于多分类问题，它将一组输出转换为概率分布，使得每个输出值介于0和1之间，且所有输出值的和为1。

$\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$
#### 神经元是怎么工作的？
1. 接受输入数据 

（比如 $x_1, x_2, x_3$…）

2. 加权求和

$z = w_1x_1 + w_2x_2 + w_3x_3 + b$

3. 通过激活函数处理

$a = \text{activation}(z)$

4. 输出结果，传递给下一个神经元

#### 🌰 是否需要带雨伞出门
神经元示例

🧩 1. 特征输入 $x_1, x_2$

| | 含义 | 示例值 |
| --- | --- | --- |
| $x_1$ | 今天温度 | 10℃ |
| $x_2$ | 是否下雨（1=是） | 1 |


🧮 2. 加权求和 

$z=w_1*x_1 + w_2*x_2 +b_1$

设定权重：

$w_1 = -0.5$（温度越低，越想穿外套），

$w_2 = 2$（下雨时强烈建议穿），

$b = 1$（偏置）

于是，神经元的加权求和值：

$z = (-0.5 \times 10) + (2 \times 1) + 1 = -5 + 2 + 1 = -2$

🧠 3. 激活函数

假设使用了 `Relu`的激活函数，那么：$\text{ReLU}(-2) = 0$

⏭ 4. 输出

输出 0，代表：不建议带雨伞出门。

### 模型参数
模型参数（Model Parameters）是机器学习和深度学习模型中的可调节变量，这些变量在训练过程中被优化，以使模型能够更好地适应训练数据并进行预测。

模型参数主要包含两种：

+ 权重 Weights
+ 偏置 BIas
### 🌰 是否需要穿外套出门
神经网络示例，一个典型的二分类问题。

![画板](https://cdn.nlark.com/yuque/0/2025/jpeg/295096/1744011028721-5816bbec-8ad8-49f0-b2df-5bfe912ce0df.jpeg)

🧩 1. 特征输入

| 特征 | 说明 | 示例值 |
| --- | --- | --- |
| 温度（°C） | 越低越想穿外套 | $x_1 = 10$ |
| 是否下雨 | 0=否，1=是 | $x_2 = 1$ |
| 是否刮风 | 0=否，1=是 | $x_3 = 1$ |


🏗️ 2. MLP 结构设计

```
输入层 (3个神经元)
     ↓
隐藏层 (2个神经元，ReLU 激活)
     ↓
输出层 (1个神经元，Sigmoid 激活 → 输出概率)
```

🧠 3. 模型计算过程

1️⃣ 输入层

$$x = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} 10 \\ 1 \\ 1 \end{bmatrix}$$

2️⃣ 隐藏层（2个神经元）

$$z^{(1)}_1 = w_1^T x + b_1$$  第 1 层， <font style="background-color:#FBDE28;color:black">温度低 & 有下雨/刮风 → 你可能冷</font>

$$z^{(1)}_2 = w_2^T x + b_2$$  第 2 层，<font style="background-color:#FBDE28;color:black">温度特别低时一定得穿</font>

设定权重：

$$W^{(1)} =
\begin{bmatrix}
-0.5 & 1 & 0.5 \\
-1 & 0.5 & 0.5
\end{bmatrix}, \quad
b^{(1)} =
\begin{bmatrix}
0 \\ 1
\end{bmatrix}$$

3 个输入特征，2 个神经元，因此，6 个权重，2 个偏置， 权重 W 会通过<font style="background-color:#FBDE28;color:black">不断试错来调整</font>

计算隐藏层结果：

$$z^{(1)} = W^{(1)}x + b^{(1)} =
\begin{bmatrix}
-0.510 + 11 + 0.51 + 0 \\
-110 + 0.51 + 0.51 + 1
\end{bmatrix}

\begin{bmatrix}
-3.5 \\
-7
\end{bmatrix}$$

应用 Relu 激活函数：

$$a^{(1)} = \text{ReLU}(z^{(1)}) = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

3️⃣ 输出层（1个神经元）

设置权$$W^{(2)} = \begin{bmatrix} 0.8 & -0.5 \end{bmatrix}, \quad b^{(2)} = 0.2$$

$$z^{(2)} = W^{(2)} a^{(1)} + b^{(2)} = 0.8*0 + (-0.5)*0 + 0.2 = 0.2$$

使用 Sigmoid 函数计算输出$$\text{Sigmoid}(0.2) = \frac{1}{1 + e^{-0.2}} \approx 0.55$$

上述 MLP 模型中，模型参数总共有： 6+2+2+1 = 11 个



### 🌰 识别手写数字的神经网络
手写数字为 28*28 像素的图片输入

![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743996645009-4679aa6b-f761-4a5d-abba-6caab5c932d3.png)![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743996667085-afb06851-882a-4258-a8c8-bb5dc6331a40.png)

🧩 1. 特征输入

每张图像都是 28*28 的矩阵，将矩阵打平，得到一个 784 个元素的一维向量。

🏗️ 2. MLP 结构设计

![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743996925125-a60b2cce-3d25-4f74-aabe-5e5c0f89f437.png)

```
输入层 (784个神经元)
     ↓
隐藏层1 (16个神经元，ReLU 激活)
隐藏层2 (16个神经元，ReLU 激活)
     ↓
输出层 (10个神经元，Softmax 激活 → 输出概率)
```


🧠 3. 模型计算过程

有多少个模型参数？

N = 784*16 + 16 + 16*16+16 + 16*10+10 = 13002

权重矩阵

$$ W_{\text{input-hidden1}} =
\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1,16} \\
w_{21} & w_{22} & \cdots & w_{2,16} \\
\vdots & \vdots & \ddots & \vdots \\
w_{784,1} & w_{784,2} & \cdots & w_{784,16}
\end{bmatrix} $$ 
784*16

$$ W_{\text{hidden1-hidden2}} =
\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1,16} \\
w_{21} & w_{22} & \cdots & w_{2,16} \\
\vdots & \vdots & \ddots & \vdots \\
w_{16,1} & w_{16,2} & \cdots & w_{16,16}
\end{bmatrix} $$
16*16

$$ W_{\text{hidden2-output}} =
\begin{bmatrix}
w_{11} & w_{12} & \cdots & w_{1,10} \\
w_{21} & w_{22} & \cdots & w_{2,10} \\
\vdots & \vdots & \ddots & \vdots \\
w_{16,1} & w_{16,2} & \cdots & w_{16,10}
\end{bmatrix} $$
16*10

一个设计好的神经网络，其权重矩阵和偏置值，也就是模型参数是固定的，因此：

+ 模型的大小
+ 模型预测花费的时间 和 cpu/ram/gpu 资源开销是固定的

不会受到训练次数的影响。

## 多层感知机 MLP 

👉 <font style="background-color:#FBDE28;color:black">MLP = 最基础的神经网络的一种</font>


MLP 是 **多层感知机**（**Multilayer Perceptron**）的缩写，是一种**前馈神经网络（Feedforward Neural Network）**，是深度学习中最基础也是最经典的神经网络结构之一。



![](https://cdn.nlark.com/yuque/0/2025/png/295096/1743990126952-2e9f4693-957a-47ce-9b96-0ad861223620.png)





## 神经网络的训练
![](https://cdn.nlark.com/yuque/0/2025/png/295096/1744011072973-8ee57064-420d-4668-b840-60c2ed21d320.png)

模型训练的过程，就是不断调整“<font style="background-color:#FBDE28;color:black">权重矩阵</font>”和“<font style="background-color:#FBDE28;color:black">偏置值</font>”的过程


### 向前传播 Forward Propagation
<font style="background-color:#FBDE28;color:black">向前传播的方向，就是模型预测的方向。</font>

步骤：

1. 输入数据
2. 计算每一层输出，并作为输入值传递到下一层
3. 最终输出层输出



向前传播的目的：

+ 计算输出
+ 为后续反向传播 计算误差提供依据

### 反向传播 Forward Propagation
反向传播是神经网络中的一种算法，用于通过计算误差的梯度来<font style="background-color:#FBDE28;color:black">更新</font>神经网络的参数（<font style="background-color:#FBDE28;color:black">权重和偏置</font>）。

核心目的：

+ 减少神经网络的输出与真实标签之间的<font style="background-color:#FBF5CB;color:black">误差</font>
+ 通过<font style="background-color:#FBF5CB;color:black">调整权重</font>来优化模型。

执行步骤：

1. **计算损失函数**

根据 向前传播的输出 与 真实数据标签，计算损失函数

$$\text{Loss} = \frac{1}{2} \sum_{i=1}^{N} (y_{\text{true}}^i - y_{\text{pred}}^i)^2$$

2. **计算输出层的误差（梯度）**

在输出层，计算损失函数关于输出层神经元输出的梯度。这个梯度表示了输出层神经元输出变化对总损失的影响。

3. **反向传播误差到每一层**

误差从输出层开始，逐层反向传播到前面的隐藏层。每一层的误差梯度是通过<font style="background-color:#FBF5CB;color:black">链式法则计算</font>的。

对于每一层，梯度是通过计算该层输出与损失的关系来确定的。

4. **更新权重和偏置**

使用计算得到的梯度，更新每一层的权重和偏置。通常使用梯度下降算法来更新权重：

$$W = W - \eta \cdot \frac{\partial \text{Loss}}{\partial W} \\
其中，\eta 是学习率，\frac{\partial \text{Loss}}{\partial W} 是损失函数关于权重的梯度。$$

<font style="background-color:#FBF5CB;color:black">一次反向传播过程，会调整每一层的权重和偏置值。</font>
5. **重复计算**


### 梯度下降
[[2.4 梯度下降、损失函数、自动求导和链式法则]]


[Neural Networks and Deep Learning（四）图解神经网络为什么能拟合任意函数 | bitJoy](https://bitjoy.net/2019/04/07/neural-networks-and-deep-learning%ef%bc%88%e5%9b%9b%ef%bc%89%e5%9b%be%e8%a7%a3%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e4%b8%ba%e4%bb%80%e4%b9%88%e8%83%bd%e6%8b%9f%e5%90%88%e4%bb%bb%e6%84%8f%e5%87%bd/)

### 模型更新的频率 Update Frequency
模型更新的频率指的是模型在训练过程中进行<font style="background-color:#FBDE28;color:black">参数更新的频率</font>。

更新是在几个层面：

+ **Batch Update** 在每个小批量数据（mini-batch）上进行一次参数更新
+ **Epoch Update** 在整个训练集上迭代一次之后，更新模型的参数。
+ **Online Update** 对于每个训练样本都进行一次更新

### 超参数 Hyperparameters
超参数是指在模型<font style="background-color:#FBDE28;color:black">训练之前</font>手动设置的参数，这些参数<font style="background-color:#FBDE28;color:black">不会</font>随着训练过程而<font style="background-color:#FBDE28;color:black">更新</font>，而是对模型的训练过程有重要影响。

常见的超参数有：

+ **学习率（Learning Rate）**：决定了每次更新时参数的调整步长
+ **批量大小（Batch Size）**：在一次更新中所使用的样本数量。
+ **迭代次数（Epochs）**：整个训练集被训练一次的次数。
+ **优化算法（Optimizer）**：用于更新模型权重的算法。
+ **正则化参数（Regularization）**：控制模型复杂度的超参数，防止过度拟合
+ **网络结构的超参数**：例如深度学习中的层数、每层的神经元数量、卷积核的大小等。

超参数调节方法：

+ **网格搜索（Grid Search）**：通过穷举法测试预设的超参数组合，选择最佳的组合。
+ **随机搜索（Random Search）**：随机采样超参数空间并评估模型效果。
+ **叶斯优化（Bayesian Optimization）**：通过构建概率模型来指导超参数搜索过程，更加高效。

## 参考资料
[【官方双语】深度学习之神经网络的结构 Part 1 ver 2.0_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.788.recommend_more_video.0&vd_source=d2c6cad4e8b48a4a5ab3df7cb838685b)

[【官方双语】深度学习之梯度下降法 Part 2 ver 0.9 beta_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ux411j7ri/?spm_id_from=333.788.recommend_more_video.0)

[【官方双语】深度学习之反向传播算法 上/下 Part 3 ver 0.9 beta_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV16x411V7Qg/?spm_id_from=333.788.recommend_more_video.-1&vd_source=d2c6cad4e8b48a4a5ab3df7cb838685b)

[neuralnetworksanddeeplearning.com | bitJoy](https://bitjoy.net/category/0%e5%92%8c1/neuralnetworksanddeeplearning-com/)

